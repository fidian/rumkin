Hiya,

I tried punching in 50 random letters (literally smashing my keyboard, making sure to only use the 26 regular letters) versus 50 times the same letter (g) here, and the entropy barely differed (223.1 bits vs 217.2 bits). Intuitively I'd say this is incorrect, as the 50 gs can obviously be represented much shorter than that, and the entropy would in fact be much lower, in the magnitude of around 10 bits rather than hundreds. Is it a possible improvement to use the following approach in your algorithm? Given a password P, Compress the given password (we'll call this C), calculate the entropy using whatever method you've been using until now (called C'), and calculate the entropy of P itself (P'), and then take the minimum of C' and P', and return that as the real entropy of P? For really "hard to guess" passwords, this compressed password will barely yield a lower entropy, or even a higher one, so it wouldn't impact the result, but for easy passwords like the 50 gs described above, it might result in a more accurate result.

Best regards,
Joran Dox


The entropy bits are based on the likelihood of letter pairs occurring in natural language.  Smashing your keyboard produces random pairs that could appear in normal words, thus it gets a lower score.  The odds that a "g" follows "gg" is extremely slim.  You also get different results if you use 50 "z" letters because there's words that have "zzz" in them (most notably the word indicating a comic character is sleeping).  I generated 5 totally random passwords consisting of only lowercase letters and got these entropy ratings, which mirror your findings.

dycivlacurcaxvtzsafqirdevtcvfmmjqvyozeacbshrqherkn 213.3
atrmkwvprutqdjzobhumfqjcubnymdxtmaxfiiutbiydmxfsyk 216.7
fdaxixrdclpkpesvkynspzrcwwtgdnqfvjjsceitcjjznhludl 218.9
uczqfgyjsaklwvucrdcqcspcprcmxmoofhodhkbozxpatqruzn 217.9
htmcuxltdavkmzvgsmfrfdbyuyldpadcauiofawvmsfezutmyr 217.9

The choice of letters can really only make the entropy rating go down, not up.  With a keyspace of 26 lowercase letters, that's log2(26) = 4.7 bits per letter.  Times 50 = a theoretical maximum of 235 bits of entropy.  However, what the algorithm does is remove some of those bits because having "h" after "t" is pretty likely.  Your example of 50 g characters is very unlikely so I don't remove much entropy from the score.  This makes English words score much lower, as you can see in this 50-character password.

itisentirelypossiblethatcomputersaresmarterthanman 189.9

Your measurement of an entropy rating has a few snags.  How would I compress C?  Once compressed, how would I calculate the entropy level?  My algorithms are based on a table that is generated from actual words.  Unless you're suggesting I build another table for compressed dictionary entries and see how it compares.

Let's run this through with the 50 "g" characters.  One could argue that 50 "g" is more strong than 5 "g"s.  Brute force crackers can get the 5 g version trivially but won't be able to crack 50 g.  Depending on the speed of the computers, quantity of machines and the algorithm efficiency/weaknesses, one could crack a 6 g or maybe up to a 13 g password in a tolerable amount of time.  5 g is weaker and should have a lower score.  10 g is stronger.  50 g is super strong.  It's also not an intuitive password - one won't likely have it protecting a system.  Usually one uses words and some more advanced password crackers look for words based on letter pair probabilities similar to what I have in my algorithm.

In the end, I feel that 50 g characters is much more secure than a compressed version (removing duplicates) with only 1 g.  I'm open to more discussion, especially if you could use some examples, arguments and calculations to help illustrate that a 50 g character password is weaker than 50 random letters.




While I'm not sure what advanced techniques hackers use, I think that there are generally four kinds of passwords I can think of right now: dictionary passwords, word-like passwords ("words" not in the dictionary but easy to remember, including names and the like), other easy to remember passwords and true random passwords.

For dictionary passwords I'd argue that the amount of words in the dictionary is the charset size, and the amount of words is really the password length. xkcd's "correct horse battery staple" would then be one possibility in a sea of 1 000 000^4 (~1e24, assuming 1 000 000 words, an overestimate of the bound provided here) rather than 27^28 (~1e40, 26 letters + space, 28 characters total)

Using bigrams for English covers the second class perfectly. I do wonder what the speed-up for cracking a password using bigram stochastics would be as opposed to a regular brute force attack. I think this class of passwords are only slightly worse than true random passwords because they typically use a smaller charset, but I'd love to see numbers on this.

Given that it's a truly random password, I believe the only option (unless you have access to hashes or other circumventions) is brute-forcing it, forcing the hacker to check charset^length passwords.

The 50g password would then be part of the third class of "other easy to remember passwords". These passwords are part of the truly random class as long as nothing is known about it, but the passwords in this class lose all entropy if the "generating function" is known. In the case of the 50g password that function is just "add g", but for example the password "112358132134" might be hard to crack until you know it's the fibonacci sequence, and then all you need to find is the correct length.

Basically what I'm trying to say is that, while those passwords are generally as safe from brute force attacks as other passwords of the same length, they're not nearly as safe when faced with an attacker who knows you, either directly or by researching you or the people close to you.

By the way, the reason I found your site was that I recently tried to sign up on a website with a throwaway password that was too short, so I just duplicated it, and then I got an error that it should not have repeats. This got me thinking and as such I ended up searching for an online password strength test.

Of course now that I've given it more though, this means that compressing still doesn't scratch the surface of possible generating functions, and thus only covers a small subset of the passwords in that class, so I withdraw my suggestion in that regard. Still, checking passwords like that is interesting food for thought I'd say.




It's a topic that could benefit from years of research because it is more complicated.  I'd wager that the estimates are wrong with any password if the attacker knows you.  I had a coworker that used the first letters of various songs or poems.  Easy to remember but looked like rubbish.  By cross referencing songs and books, I could guess millions of passwords pretty quickly.  By listening to his keystrokes I could count the number of characters.  It wouldn't last long had I tried.  :-)

Regarding your numbers about the dictionary words, that's exactly the type of password I'm attempting to estimate.  I think I might add other entropy estimators, such as one that does the theoretical maximum with the given characters using only those characters in the character set.  Then I can show what is the maximum one can get, a breakdown of the different methods and perhaps give them the lowest score out of all of them.  It's certainly food for thought in the future.  I've been making notes because of this conversation on ways I can make the page more informative and the checking more thorough.

I appreciate your feedback and I'll keep it around for a while to hopefully motivate me to finish redoing my website.  :-)




RE:  http://rumkin.com/tools/password/passchk.php
 
I’ve referred colleagues to this article as an example of how passwords can be graded on strength, and it appears to work OK.  HOWEVER, on a whim I entered a string of "a" and then a string of “1” and eventually it shows up as STRONG for both.  Seems unlikely any password of a single character could be considered anything but weak.  Am I missing something?   Would a cracker really have a problem with 32 of the same numeric or 15 of same alpha character?  Both show over 60bit.  In fact, 16 length of “ab…” wasn’t as strong as 15 length of “a…”
 
For what it’s worth.
 
-- Jim Moomaw 
   Intel Corporation



My strength algorithm takes into account the English language, which doesn't typically mix numbers into its words.  In essence, having a random number after your word increases the entropy by about 2.33 bits.  I do understand that having a bunch of 1's doesn't seem too powerful (and I probably should look into the probabilities of numbers repeating themselves), but the length also works in your favor.  Imagine if my pasword is mere a 10-digit number:  1111111111.  I doubt people would randomly guess it when they are trying their password cracking programs.  Making it 32-characters long makes the cracking nearly impossible.  Wordlists are relied upon heavily when password breaches expose hashed passwords, so the fewer words or word-like things you have in your password, the better off you are.

I'll make a note to revisit the amount of entropy numbers add, but the simple formula assumes that you are picking random numbers instead of repeating them.


--
Tyler Akins



I am utilizing your method for approximating entropy of passwords as a method for security.  However, I am curious as to your source for English bigram probabilities.  I am having trouble finding the source that you would have used since you have non 0 probs for every letter combination.  Was it smoothed in some way?
Max
Cornell University
Computer Science Dept.



No, it just apparently had all sorts of uncommon words in it.

--
Tyler Akins







I looked at your webpage on password strength, passchk.php.
 
Just a thought:  the ubiquitous correcthorsebatterystaple should only have 72 bits of entropy.  Why?  Because when you start typing it into google search, Google suggests the phrase after only 9 characters.  Any phrase from a common song, will typically have only 96 bits of entropy.  Of course, substitute or insert some *non-common* letter, number, or symbol substitutions, and your entropy probably goes back up.
 
In line with that, when you substitute google-suggested words in for characters, a lot of supposedly hard passwords will come out easy. 
 
 
Point being, there may be yet another limitation on what is a good or bad password.




I don't know about "any phrase from a common song will typically have only 96 bits of entropy" but it is true that I don't tie into Google for the results.  My checker only checks against the strength of the letter pairs and how often they pop up in normal phrases.  For instance, the "u" following a "q" provides almost no entropy.  Google goes a bit further and breaks it up by word and by the next most likely word as well, which is beyond the scope of a mere tool in JavaScript.




Absolutely.  Nonetheless, the hint might be well included in the “password hints”.  Suggest that people start typing in their passphrase to google search, and if Google suggests the phrase after 8 letters, then their passphrase is no better than an 8-letter pass phrase.



I don't think that's entirely true.  Let's say your password does pop up after 8 characters, but it is a 10 character phrase.  No password crackers out there have the searching capabilities of Google and nobody will run their dictionary through Google to see what could pop up.  Even if they did, the 8 letters that trigger the password might work for you but might not work for me since Google tailors the search results as well as the autocomplete results to the individual searching.  At the worst, the password crackers would add that word to their wordlist, but most likely that would be too much trouble and they'd just use a wordlist mutator that concatenates wordlists together, making one that is *insanely* long.  Or, they'd brute force crack your password and then each character would make it significantly harder to crack.

All that argument aside, I will add it to the website because it is still useful advice.  We don't want anyone, not even Google, to be able to guess your password.  I've committed a change to my website and it should show up in the next hour or so.  Thanks for the useful suggestion and sorry for being grumpy when you first proposed it to me.

I've put some of these numbers on a website for others to peek at:  http://www.fidian.com/programming/passwordsecurity




Tyler, 

Nice of you to make that tool available.  However, it appears you are not counting any entropy for the first symbol, but only summing the bigram entropies.  Two random lower case letters have more entropy than 4.7 bits.  

I tested an 11 character password with 8 random lower case letters and three random symbols in a cluster.  Your program tells me 44.5 bits.  Without even trying I know that 11*4.7 is a significant underestimate.  I would probably add log2 (11 choose 3) at least, which makes a reasonable estimate closer to 60 bits.  

The easy tweak is just to add the entropy from the first symbol. 

Allan





I have found that there is almost no entropy added from the first letter when using my method of calculating letter frequencies.  I used text and dictionaries, and people tended to create words to cover most of the letters fairly evenly.  As for your 11 character random password, let's assume that it's purely random.  There are 26 possibilities for each character, which is about 4.7 bits per letter.  This assumes the password is truly random.  My frequency charts will score "the" and "tho" as being more likely than "yyw" or "qmk", thus providing less entropy than pairs and triplets that do not occur randomly in English words.  So, 11 truly random characters should give you 4.7*11 bits of entropy, which is about 51.7 bits.  I'm guessing you either mashed in something that looked like an English word at points or was potentially partially used as a common password, providing you a weaker score.



Tyler,

I must politely disagree with your assessment.

I'm not some random duffer here (or duffer of randomness).  I've been
living in the big H world for thirty years.  I developed a Chinese
language text input system for the IBM PC between 1985 and 1991 which
involved phonetic input, a character frequency N-gram model, and some
parsing to handle number measure-word expressions.  In the early 1990s
I developed an original algorithm for encoding 0/1 word trigram tables
used in speech recognition, which unfortunately requires GPU level
compute performance to perform the static encoding, so I had to let
the project languish on the back burner for a long time, even though I
never really stopped thinking about it.   Recently I've begun working
on this again.  Shannon entropy is in my blood.

As described, my sample 11 character password contained random lower
case letters and a cluster of three random symbols.   There are more
than 26 symbols, but we'll conservatively treat all 11 symbols as
having cardinality of 26.

11 * log2(26) = 51.7

This doesn't count the entropy of character class placement.  choose
(11, 3) = 165.  log2(165) = 7.4 bits.  51.7 bits (per symbol in-class
entropy) + 7.4 bits (class placement entropy) = 59.1 bits total
entropy.

I've given the attacker credit for knowing that the class division is
8 lower case and 3 symbols (with zero upper case and zero digits).  I
haven't given the attacker credit for knowing that the three symbols
form a contiguous interior cluster.  Conservatively, that seems like a
wash to me.  Four of my lower case letters came from etaoinshrdlu.
The other four came from the bleacher seats.

Your program gave me 44.5 bits.  That's just not right by any stretch
of the imagination.

I happen to have a _very_ large list of real world passwords somewhere
in my possession as I'm quite interested in these things.  I wouldn't
be surprised if not a single four character substring from my test
password is found anywhere in this dataset large enough to provide a
unique password for every citizen of a G7 minnow.  This kind of thing
really shouldn't be floating around out there.

Concerning the eight letter password formed by deleting the three
interior symbols, I just checked with
/usr/share/dict/american-english.  One three letter substring found
some matches.  No four letter substring matched any word.  One two
letter sequence matched no words at all.

Your theory that my random letter sequences are treading on natural
language doesn't hold water.  You'd have better luck predicting my
string by training on the Perl CPAN archive.

If your program is valid for a restricted subdomain of password
structures, then you should make this clear rather than misleading
people with estimates far more approximate than people who've never
seen a big H might reasonably suppose.

It's a mere 20 below and you're sending people outside in sweltering
seal-hunting parkas.

Allan







Dear Tyler
I'm generating passwords using PasswordMaker [1] and I've run into
some strange results. Consider the following password:
J\X[8x+|Tw{3_#^3Ml

Your password strength test gives the following (sound) results:
Length: 18
Strength: Strong - This password is typically good enough to safely
guard sensitive information like financial records.
Entropy: 82.7 bits
Charset Size: 92 characters

And [2] agrees:
Score:  100%
Complexity:     Very Strong

However, if I input an apparently very insecure 18-char long password:
aaaaaaaaaaaaaaaaaa

Then your tool suggests:
Length: 18
Strength: Strong - This password is typically good enough to safely
guard sensitive information like financial records.
Entropy: 75.6 bits
Charset Size: 26 characters

While [2] disagrees:
Score:  0%
Complexity:     Very Weak

Is this an issue with your strength measuring tool, or is such a
password genuinely safe to guard financial records?

Regards
Liviu

[1] http://passwordmaker.org/passwordmaker.html
[2] http://www.passwordmeter.com/



Each strength testing tool will gather statistics and calculate strength using different algorithms.  My password strength tool uses groups of letters and information about letter patterns to determine the score.  For instance, if you have a q, then it is most likely followed by a u.  If you have "th", then the next letter is probably a vowel.  The likelihood of "aa" being followed by "a" is tiny.

The entropy for mine would go up when you use more letters.  I see a lot of symbols in your message.  5 uppercase letters, 2 lowercase, 3 numbers and 8 symbols.  This disproportionate favoring of symbols adds less entropy because the pool of random symbols is smaller than random letters.

Also, mine works with the theory that the password you don't need to write down is far more secure than the password you are forced to write down.  "Smiles is the longest word; there's a mile between the s's" is a lot easier to remember than "u*m9THu?24ys[1xcTH".

--
Tyler Akins




Hello Tyler
Thank you for all these explanations.


On Tue, May 22, 2012 at 6:19 PM, Tyler Akins <fidian@rumkin.com> wrote:
> Each strength testing tool will gather statistics and calculate strength
> using different algorithms.  My password strength tool uses groups of
> letters and information about letter patterns to determine the score.  For
> instance, if you have a q, then it is most likely followed by a u.  If you
> have "th", then the next letter is probably a vowel.  The likelihood of "aa"
> being followed by "a" is tiny.
>
I see. It makes more sense, although it is still slightly scary that
such a potentially common password be classified as very secure.


> The entropy for mine would go up when you use more letters.  I see a lot of
> symbols in your message.  5 uppercase letters, 2 lowercase, 3 numbers and 8
> symbols.  This disproportionate favoring of symbols adds less entropy
> because the pool of random symbols is smaller than random letters.
>
The actual pool of symbols used for generating this specific password was:
ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789`~!@#$%^&*()_-+={}|[]\:";'<>?,./


> Also, mine works with the theory that the password you don't need to write
> down is far more secure than the password you are forced to write down.
>  "Smiles is the longest word; there's a mile between the s's" is a lot
> easier to remember than "u*m9THu?24ys[1xcTH".
>
Well, that's the beauty of PasswordMaker generated passwords: you
don't have to write them down.

Their approach is described here [1], but to sum up: you choose a
strong Master Password and then you use PasswordMaker to generate, via
hashing algorithms, very long and unique passwords which
cannot---computationally infeasible---reveal the MP. Then to log in to
a given on-line or off-line account, you recreate the password
on-the-fly and insert it in the required field. This approach is much
better than using one secure password throughout, although some
security risks (say, copy/paste password) do exist.

For example, to generate the password below,
J\X[8x+|Tw{3_#^3Ml

use [2] (although many other clients are available) and the following settings:
Input Url: yahoo.com
Master password: asdf
Length: 18

For google.com the password would be:
Kf/6=cyf.g|2>Ww2lw

Regards
Liviu

[1] http://passwordmaker.org/
[2] http://passwordmaker.org/passwordmaker.html




> have "th", then the next letter is probably a vowel.  The likelihood of "aa"
> being followed by "a" is tiny.
>
I see. It makes more sense, although it is still slightly scary that
such a potentially common password be classified as very secure.

It's not that common.  How often have you seen a 16-letter password that is all "a" characters?  I've downloaded many wordlists, common password lists, password leaks and other related materials.  I don't ever recall seeing aaaaaaaaaaaaaaaa once.  I've seen abc123 and similar, but always short.

> The entropy for mine would go up when you use more letters.  I see a lot of
> symbols in your message.  5 uppercase letters, 2 lowercase, 3 numbers and 8
> symbols.  This disproportionate favoring of symbols adds less entropy
> because the pool of random symbols is smaller than random letters.
>
The actual pool of symbols used for generating this specific password was:
ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789`~!@#$%^&*()_-+={}|[]\:";'<>?,./

I meant the pools in my program.  I lump uppercase into one, lowercase into another, numbers into a third, and "the rest" into a fourth.

I do understand that the PasswordMaker program (and many others out there) create cryptographically strong passwords.  When you are limited to eight characters, those programs will produce better security than using a random word and a number after it.  However, if you are given the opportunity to use long passwords, then the advantage is minimal.  Yes, you can get far more entropy with a truly random password, but people type in words faster and even could memorize phrases far easier than truly random passwords.

Well, that's the beauty of PasswordMaker generated passwords: you
don't have to write them down.

Well, you sorta do.  You need them in a program or generated by a program.  I don't have to even go back to the previous message to remember what I sent you earlier regarding smiles being the longest word and I can type it in using my phone, my web browser, or my neighbor's computer.  If I tried to type it in now, I may not be 100% accurate, but it wasn't something that I even tried to remember.  I honestly thought I wouldn't get a reply!  :-)  So, even though I didn't try to remember the password, I'm going to be close with this:  "Smiles is the longest word; there's a mile between the s's" (... checking ... wow, I surprised myself and got it right).  I dare you to do the same with the password you tested and emailed me originally.  With this one I've got a length of 58 and a reported entropy of 280.3 bits.

I'm certainly not trying to dig on PasswordMaker or those style of password managing tools.  The practice of using a different password for each site is also a really good thing.  I just get annoyed when I can't figure out the password to a web site because I'm not using my computer or I'm not using the plugin or whatnot.  Yes, there's the web version of most of these tools, but it is one more thing that I need to depend upon.  What if the website happens to be down, the algorithm changes, the hostname changes or I messed up the settings?

My tool is just to help people test passwords and it certainly isn't the "end all and be all" of tools out there.  I also like that you are using PasswordMaker to manage passwords.  Most people don't care at all!




Hello,

I've tried out your tool to check the strength of passwords. I wanted to use it in one of our projects but I've got a question. I've entered the password 'KKKKKKKKKK' and got as result that the strength is reasonable. I would say that this is no good password. Why gets such a password such a high value for the entropy? Is there something wrong in the script?
I hope you can help me (unfortunately I don't know how old this script is and if you still maintain it).

Thanks and regards,
Michael



Well, it is reasonable because k doesn't appear after another k too often.  Plus, wordlists won't have a kkkkkkkkkkkkk listed, and I probably didn't even get the number of k's in that word correct.  While businesses may have "no repeat letters" in their password security policies, having just k's would be difficult to guess unless I saw you typing it at a keyboard.



After visiting your site I'm fixated on entropy. I'm confused by how your tool calculates it though.

I skimmed parts of the NIST's Authentication Guide you linked to. 
(relevant: pp. 49-50)

As I understand, entropy is influenced by:
length
generation (user/random)
character set
dictionary test bonus
composition test bonus
Is there something I missed? I'm playing with the strength test, trying out memorable passwords in hopes of exceeding 75 bits. I tried to trial-and-error which characters have a higher entropy estimation (bit). 

Using your calculator:
jjjjj 18.6 
ttttt 16.9

Same 26 character set, same length and bonuses should(n't) apply equally.

What causes the variation? Are certain characters different bit values eg. vowels are lower due to their commonality? (Still wouldn't explain J vs. T.)

Best,
Mike





It's because you're more likely to get two t's after each other than two j's after each other.  It basically boils down to the amount of entropy of the second letter in the word.





Hello,

 Nice work on your pass check program, it is great to see a practical implementation on entropy guessing. What you've done with a few KB of code is really impressive.

 I did some playing around, and found some odd results: a password aaaaaaaa is four times strong (e.g. 31 to 29 bits) than zzzzzzzz. I'd understand if you rated them equally, but otherwise I expected: (1) a to have less entropy than z; (2) repetition/patterns/etc should be quite low entropy overall. 

 Anyway, please accept this in the spirit it is intended, as I think you've done a really nice job! :)



It turns out that the dictionary I had had fewer aaa's than zzz's listed, so the probability of getting another z after two z's in a row was higher than getting another a after two a's in a row.

That was an interesting find you relayed to me, and it made me wonder about what I was doing wrong to get those probabilities.




Hello, I have used your application \"Password Strength\" (http://rumkin.com/tools/password/passchk.php) and would like to know which is the algorithm that you used for generating the value of the entropy of a password. I know the Shannon algorithm and I tried implement it, but my values of entropy are much lower than those calculated from your program. I read that you lumped together all non-alphabetic characters into the examination group and for this reason the entropy score will be lower.

The algorithm that I used is this:

http://upload.wikimedia.org/math/8/7/e/87efdf0d38947240683250d3a24466e0.png



Thanks.




I had made my own algorithm based on postings to various crypto newsgroups and on other web pages that I had found on the internet.  I had intended to describe how the algorithm on the page, was the explanation inadequate? If so, please feel free to ask more questions.

Basically, I had analyzed a bunch of text to get letter trigraph frequencies so I could predict the next letter based on the previous two letters.




The explanation was very useful, the problem is that I can not find an "universal and valid" algorithm for calculating the entropy of a password, and then seeing that I could contact you I decided to write in the hope that I will find a answer to my question. I state that are new to cryptography, and therefore do not have much knowledge about this field.
Searching in Internet I found several algorithms, but I always obtain a very low levels of entropy or at least special values.
For example, I found an algorithm in which the value of entropy, to obtain a high value of entropy, was multiplied by the length of the password. Another algorithm, to increase the final value of entropy, ie that seen by the user, the value of the entropy multiplied by 100 (therefor a value of entropy equal to 4.88 multiplied by 100 would become 488, that is to big). Yet another algorithm for calculating the entropy of a string, consider the distance between ASCII character.

 My algorithm:

entropy = 0
length = string_length (argument0)
for (i = 1 i <= 255 i = 1)
{
p_x string_count = (chr (i) argument0)
p_x if> 0
p_x = entropy / length * log2 (p_x / length)
}
entropy = abs (entropy)
return entropy

My algorithm for a password like "Taqobic8Enufisep", returns a value of entropy equal to 3.88 (probably wrong).
I am aware that there must be a relationship between the character set which is generated password, and the characters in the password. For this reason I was looking for an algorithm applicable to the world of IT security. That is an algorithm suitable for calculating the number of probabilities that a bruteforce program used to find the password. I know that Shannon's formula allows you to do this, but I can not explain why there are many programs that return a value of entropy so high, when the rule of Shanon returns very low values.





There are some errors in my algorithm:

entropy=0
length=string_length(argument0)
for (i=1 i<=255 i+=1)
{
p_x=string_count(chr(i),argument0)
if p_x>0
entropy+=p_x/length*log2(p_x/length)
}
entropy=abs(entropy)
return entropy

the + sign was disapeared.
I created a new algorithm that takes into account the character set (AZ) and (0-9). The variable cs contains the total number of symbols AZ and 0-1.
cs=62
length=string_length(argument0)
entropy=0
for (i=1 i<=255 i+=1)
{
p_x=string_count(chr(i),argument0)
if p_x>0
entropy+=p_x*log2(cs)
}
return entropy;




A lot of implementations of the entropy calculations on the internet are flawed.  It turns out that what you are trying to calculate is the amount of entropy (measured in bits) that is produced by a particular password.  One could say that if you have A-Z and 0-9, then each letter you pick produces 36/256 bytes of entropy (if I remember correctly).  However, that is flawed because letters are not picked at random.  The letter Q is almost always followed by U, thus having a U after a Q produces almost no entropy.

My algorithm is not based on a published standard or carefully reviewed process, but it does seem to have favor among crypto people on the net.

It looks like your algorithm bases the amount of entropy a particular letter produces on how many times it appears.  Mine would give a high amount entropy for QKQLQOQNQFQGQH and yours might give it significantly less because of the repeated Q characters.



Thank you for your kind explanation :)









Your password strength testing page and code is very interesting and
definitely well done. I took a slightly different approach to my calculation
of password entropy in terms of bit strength for a system I am working on,
although much of the concept is similar. If you have a moment, I had a few
questions about some of your calculations, and perhaps a few suggestions you
might be interested in...

1. You don't seem to apply any "credit" in terms of bit strength for the
first character of the password. You only add to the entropy for the 2nd
through the last characters based on their frequency following the previous
characters (i.e. second order statistics). Is their a particular reason you
opted to not count the first character? NIST documents and related research
seem to suggest that most entropy is contained in the first character. FWIW,
I use first-order statistics (i.e. frequency of English letters in text
instead of letter pairs) when adding the bit strength for the first
character. You may want to add this to your calculations.

2. You are calculating the theoretical per character bit strength (i.e. for
a random password) with the statement:
        charSet = Math.log(Get_Charset_Size(pass));
Is there a reason you chose to use the natural logarithm instead of the
base-2 logarithm? Since the goal is to approximate the entropy of the
passwords in terms of bits, it is my understanding that you'd want the
base-2 log (which is what I use) instead of the natural log. Since
JavaScript only has a natural log function, you might want to use:
        charSet = Math.log(Get_Charset_Size(pass)) / Math.log(2);

3. Is there a specific mathematical reason why you chose to reduce the size
of the per character bit strength based on the square of the probable
likelihood of the non-occurrence of the character? I was unable to find any
research or established theories on this subject. In case your interested,
my approach here was a bit different. I compare the amount towards the bit
strength that a particular character contributes to the overall bit strength
and if great than the average amount, I use the ratio of the average to that
particular character to determine the factor to apply to that character's
bit strength. (It sounds much more complicated than it really is.) As a
result, less common letter pairs can approach the theoretical random bit
strength, whereas more common pairs are much more heavily reduced in
strength. I'm mostly curious about this, because it would have been great to
find some well researched mathematics on this subject, but I have not seen
any.

4. I'm using the text of several classic works of literature for my
character frequency tables. Out of curiosity, what did you use for the
source of your frequency tables?

5. I have been considering breaking up the symbols into common vs
less-common symbols as far as computing the character set size. I see that
you are doing this, but chose to split them based on those over the numeric
keys vs all others. I'm curious why you made this choice. I would suspect
that non-shifted frequently-used symbol keys might be more common than
others. IOW, I highly suspect people use '-' or '.' far more often than '^'
in their passwords, just because they're used to those characters and
they're easy to type. Obviously, I have no real way to know this with any
certainty.  Also, as a side note, you add 20 to the number of characters if
one of the symbols that are not over the number keys is selected, yet there
are 22 of such symbols. I didn't know if this was intentional or an error
you might want to know about.

Overall, I must say that your algorithm seem quite good and much more robust
than most others on the Internet. Other than Shannon's research into entropy
and related NIST documents, few people seem to bother attempting to estimate
actual bit strength of a password. Like you, I'm using it as 1 of the
criteria applied to determine if a password is strong enough for use on my
systems.


Regards,
Ian



Hello Ian!  Thanks for writing me about my password strength tester.  This tool is more of a toy for me than something that relates to my work, so I don't get a lot of time to make it better.  I did have a goal of making it useful and robust.  I'm glad you like it and I really like the discussion about the weaker points of my code.

1.  I don't credit the strength of a single character.

I thought I did give value to the first character, but upon checking the page I found that I was wrong.  I have added it to my list of things to fix, but I have no idea when I would have the time to update that.

2.  Natural log vs. base-2 log

The security related forums posted calculations that used natural log, not the base-2 log.  They are smarter than me for calculating things like this, so I just replicated the formula in JavaScript.  Maybe I should revisit the formulas that are out there somewhere and see if they are now using the base-2 log.

3. Square of the frequency of the letter pair

First off, this algorithm is all mine.  I couldn't find anyone who was smarter than me who provided something like this at the time.  When I was working on it, I was thinking of the letter Q.  Almost all of the time it will be followed by U.  Something like 99.8% of the time.  So, if you had a Q, you almost always had a U next, thus the U is highly predictable, thus it provides almost no additional entropy to your password.  With that in mind, I was, going to do something like (1 - probability) / 1.

When testing, this didn't really work out that well.  Tweaking the algorithm to square the probability actually produced numbers that felt better and were more in line with other research in the field.  Again, I admit that I'm not the brightest bulb in the box when it comes to hard core statistical analysis of words, but I can usually follow what other people are doing and can drop it into a simple web page with only a couple errors/typos.

So, basically I just guessed.  I wish I had a better algorithm, and yours sounds like it was thought out a bit more.  If you don't mind, would you explain a little more about yours, or perhaps you could tell me if I have it right?  I believe you are saying you calculate the probability of each letter coming up, then with each letter you will see if its probability is bigger than "average" (how is this average calculated?  mean of all the letter probabilities?) and add the ratio of the letter's probability to the average probability toward the whole entropy value.  Do you use letter pair frequencies?  What do you do if the probability is less than average?  Also, just to be clear, the probability of U coming after Q is pretty high - that's above average in your calculations, right?  So, the amount added should be (probability_Q / probability_avg) or similar?

4.  Corpus

I worked at a university for a while and had access to a couple large corpuses.  I also mixed in many Gutenberg books and English wordlists from several sources.  Apparently, people actually pay for large collections of statistically skewed random words or word frequency charts.  I built my letter frequency tables from those, and the frequencies didn't vary much from the corpuses separately or just using English wordlists or the Gutenberg books - they all basically had the same distribution for the letter pairs.  An idea here for another set of pages on my site (the cryptography pages) would be language fingerprinting based on letter pair frequencies...

I am toying with downloading and parsing a copy of the wikipedia for my letter pair frequencies instead.  I'd really like to have a large corpus and the Wikipedia provides a fairly tolerable one, at least for my purposes.  Plus it would be freely distributable, which would be a step up from what I have now.

5.  Symbol sets

I broke them into two sets because that's the way some password crackers work.  Not really complicated, but I also didn't really want to add 32 characters to the charset because someone included a dot.  I did download the 150 million leaked passwords (UserAccount-passwords.tgz), which provides an amazing look into commonly used passwords, and I suppose I should do some analysis on the commonly used symbols in passwords using that reference.

I didn't see where you were talking about 22 characters in a 20-character string.  If you are looking at line 151 of the JavaScript, the \\ gets translated into just a backslash and the \" turns into just a double quote.  Thus, 20 characters in that string.

Other people have emailed me about this very same page, asking a load of questions in a very disrespectful tone.  Initially, when I glanced at your email I saw what the topic was about and put it off for a little while before answering it.  How pleasant it turned out to be!  Not only do I really like that you emailed me about topics where you obviously expended some thought, but you also found a couple bugs that hadn't been caught since the page's inception (maybe 5-8 years ago?).  Thanks again for emailing me about the page, for pointing out the errors you found, and for the discussion about algorithms.




Tyler,

I'm glad you appreciate my comments and suggestions regarding the password
strength tester. It's a shame to hear that others are so disrespectful when
communicating with you. It's obvious you put a lot of effort into this work
and it really is quite well done. I guess that's just one more thing you
have to put up with when you have a blog or other similar public site.
Outside of business, I have a couple of blogs I've slowly been working on,
but I never seem to have the time to devote to them. So far, I've received
mostly positive comments, but there's really very, very little content on
the sites right now, so they've really not been very widely viewed. I do get
my share of spammers, but that's to be expected. I imagine I'll be more
prepared now for the negative comments, although I really do like it when I
receive good, positive and/or constructive feedback.

You also shouldn't knock yourself so hard, because IMHO your code is very
good and clearly well thought out. For me, the password testing is just one
small part of a much larger library of code for a project I've been working
on. As part of user account creation, I've wanted to add a password strength
test for quite some time, but have been putting it off. I put together a
very basic bit of code that only did length, character set size and
dictionary tests, but always knew I needed to add entropy (bit strength)
measurements.  A couple of weeks ago I finally got around to studying it a
bit.

After a bit of research, I started with NIST's Special Publication 800-63,
"Electronic Authentication Guideline, Appendix A: Estimating Password
Entropy and Strength"
(http://csrc.nist.gov/publications/nistpubs/800-63/SP800-63V1_0_2.pdf) as
well as C.E. Shannon's "A Mathematical Theory of Communication"
(http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf).  These were
definitely quite heavy with mathematical theory and many, many details that
I really didn't have the time to study (nor wanted to). I do have a
background (as well as a degree) in Applied Mathematics and Statistics,
although it's been quite some time that I've delved into this subject in
such depth. I got what I needed out of it, and started building my code from
it.

As for some of the points we've been discussing...


Natural vs Base-2 Logarithm

Throughout Shannon's work, the logarithm is used primarily without
specifying the base. In addition, he uses an arbitrary constant K to adjust
the results to the appropriate range for the particular application. His
formula are fairly abstract, and were for data compression and transmission,
not specifically for bit strength testing of passwords. For the purpose of
estimating how many random bits a password is equivalent to, NIST documents
make it fairly clear that the base-2 log is needed. I suspect that many
people are just assuming that the 'log' in any of these formula are the
natural log (which it is if you don't specify otherwise), without
understanding the math behind it. After working with Shannon's formula quite
a bit, I'm convinced the base-2 log is what should be used. I imagine many
of those posting on the security related forums don't realize this. Plus, it
doesn't help that JavaScript only has a natural log function! FWIW, I code
primarily in php (and that's what my password strength function is in), and
earlier versions of php didn't support arbitrary bases for use with the log
function either. However, you can get the log in any base using the natural
log of the number divided by the natural log of the base, i.e.
log(num)/log(base).


Square of the Frequency of the Letter Pair

As you discovered, I also saw there is virtually no one who has done any
appropriate work on this subject. Your algorithm is really the only logical
method I've seen, apart from my own approach. I was mostly curious about why
you used the square because I thought it might be based on someone's
mathematical concepts and/or research into the subject. Considering neither
of us seem to know of such work, I certainly understand your approach and
think it is well thought out. As to whether or not it is right, I can't
really tell you, because I'm not 100% sure myself as to the best way to do
this. I will explain how I'm doing it, and why. I'm very open to any
criticism (both positive and negative :-) as well as any suggestions.
Obviously, if any this helps your code in any way, feel free to use...


My Bit Strength Calculations

I based my algorithm on some of my understanding of Shannon's work. I'll
first explain a little of the relevant portion of it and then how I applied
that to my calculations.

Based on his formula, using the probability of the characters, the bit
strength of one character (in pseudo code :-) would be...

    bits = 0
    for (char = 'a' to 'z')
        bits += probability(char) * log(probability(char)) / log(2)
    bits = -bits

IOW, we sum up the probability each character occurs multiplied by the log
(base-2 for our case) of that probability. You'll note that the end result
must be multiplied by -1, which is necessary because applying log to a value
less than 1 results in a negative value. (Yes, I know I could have just used
-= when adding to the bits in the for loop, but I thought that might
distract from what was being calculated here).

The result of this calculation gives us the total bit strength for a
randomly selected character from that set of 'a' to 'z'. Now, if these
characters were completely random, the probability of any one would be the
same as the others and using the above calculation for these 26 characters
would yield the exact same value as log(26). However, because the letters
aren't fully random, the above calculation yields a value slightly less than
log(26), which correctly indicates that the bit strength of a letter chose
from a..z is a bit weaker than if they were total random.

In the best case scenario, a completely random password could at most
contain this theoretical maximum number of bits of strength for each
character in the password. However, since some characters (and character
pairs) occur more frequently than others and are therefore more likely to be
"guessed", as you did, I wanted to "weight" this maximum per character
strength based on the character frequencies. In addition, I believe that
characters with low occurrence probabilities should be treated more or less
like random characters, and thus should equal or at least approach this
theoretical maximum.

So, for a particular character, I take the probability that character occurs
(p), calculate p * log(p) and then compare that result to the average of all
such values. If it is larger than the average, I use the ratio of the
average to that value as my factor. Thus, if a character contributes 3 times
to the total bits in the above calculation, only 1/3 the actual entropy will
be credited for that character. If it is not larger than average, I
basically use 1 as the factor. In theory, a truly random password, which
does not use common letters or common letter pairings, should approach the
theoretical maximum bit strength, i.e. length * log(size_of_charset) /
log(2).

Some additional pseudo code might help clear this up...

    average_entropy = 0
    for (char = 'a' to 'z')
        average_entropy += probability(char) * log(probability(char)) /
log(2)
    average_entropy /= -26

    max_entropy = log(size_of_charset) / log(2)

    bit_strength = 0
    foreach (char in password)
        char_entropy = -probability(char) * log(probability(char)) / log(2)
        if (char_entropy > average_entropy)
            bit_strength += max_entropy * average_entropy / char_entropy
        else
            bit_strength += max_entropy

Of course, my actual code is a bit more involved than this. I do use first
order statistics for the first character (frequency of occurrence of the
characters) and second order statistics for all others (frequency of
character pairs). Also, like yours, my statistics include 27 entries with
the additional entry used for non-alpha characters. I have a table of
probabilities for the first order statistics, as well as 27 tables for the
second order statistics. Since my code is in php, I also have a few bits and
pieces to work around some php-specific floating point issues, etc.

Using the letter 'q' as an example, the statistics I'm using has 'q'
followed by 'u' virtually always (0.994975). Again, based on the stats I
used, the only other letters that might follow the 'q' are 'r' (0.000228),
's' (0.000228), or others (0.004568). As a result, the calculated maximum
bit strength for any character following a 'q' is 0.0482715, and the average
is 0.0017878.  When I first looked at these figures in my test spreadsheets
I thought it would not help in my strength calculations, but it actually
works out quite well. These values are not used in the direct calculation of
the bit strength, but rather only to determine the proportion to be used of
the real bit strength (based on the actual character set width in use). As a
result, for any character other than 'u', 'r', 's' or non-alpha, their
probability is 0, the calculated amount is less than the average, and thus
they are treated as if they are random characters and get the max_entropy
amount. Again, I think this helps yield more accurate results. However, for
a value such as 'u', the resulting ratio is 0.2472306, and thus a bit under
a quarter of the possible maximum entropy is added.

I hope this makes some sense!


Some Issues I Still Need to Address

It definitely sounds like you had much better access to good statistics than
I did. I used publicly available stats, although eventually I suspect I'll
have to put together my own stats. Like you, I just don't know when I'll
have time, so it'll likely be on my todo list for quite a while. I like your
idea of using wikipedia as a source, although that sounds like a fairly
large project!

I'm still not 100% convinced that the weighting for letters such as 'q' as
discussed above is accurate or not. While common sense does seem to suggest
there should be virtually no credit in terms of bit strength when 'qu' is
used, on the other hand perhaps some credit is worthwhile? In a brute force
attack on a password, unless the existence of the q in a given position is
known, the mere inclusion of 'qu' in the password would not truly reduce the
increase in entropy that it adds to the password. The brute force attack
would still need to try all letter in those positions, and full bit strength
for these positions would be applicable. However, for a social engineering
or dictionary-based attack, clearly a the 'u' in a 'qu' pair might add
little to no value to the password. In the end, the goal is to estimate the
equivalent password strength, so for now I'm okay with the values I'm
getting. Ultimately, it might be better if I applied some sort of curve
rather than a direct linear ratio to address such high probability pairs.

As far as the non-alpha characters, I'm heavily leaning towards using the
frequency weighting for only the letters a-z. Without a really good
statistical analysis of the actual use of non-alpha characters in passwords,
I am starting to be of the opinion that they should be treated as random
characters. The statistics I have ignored all symbols and digits, but did
include spaces, which value I have been using for all non-alpha characters.
I am now thinking this logic is wrong, and that any such stats regarding
non-alpha chars in English text doesn't really apply to passwords. Perhaps
if I were to build my own statistics in the future, it might be worthwhile
to look at the probability of a word ending with a particular character.
Determining the amount of credit to apply for a symbol or digit could be
based on the likelihood the previous character was the end of a word.

When it comes to the symbol set, I'm still torn on this subject. I don't
know much about the common password crackers, but perhaps that is something
I should look into. Do you know that they definitely separate the 10 symbols
above the number keys as a separate group? If they do, then I agree with you
that they should be as separate character ranges. Personally, I still
suspect characters such as '.', '-', etc. are more common than say '^'.
Perhaps it might be interesting to split the symbols in various possible
combinations and comparing them against that large collection of passwords.
I do agree with you that adding 32 characters just because someone used a
'.' isn't ideal. An interesting question though remains: Supposed the 32
symbols are split into a "common" set of 10 characters and "less common" set
of the remaining 22. Obviously, if a common symbol is used, only 10 is added
to the character set size. However, if a less common symbol is used, is it
correct to only add 22 to the character set size, or is the full 32
warranted in this case? If not, then it might suggest that a password using
2 less common symbols might be weaker than a password with 1 common and 1
less-common symbol, when in reality that's likely not the case. Definitely
something interesting to consider.

BTW, as far as the 22 vs 20 characters in your code. The line in question
contains the string "`~-_=+[{]}\\|;:'\",<.>/?". Discounting the 2 escapes,
I'm still counting 22 characters. Maybe I'm looking at the wrong line. In my
code I account for 26 uppercase chars, 26 lowercase chars, 10 digits, 1
space, 32 symbols, 33 control characters and 128 extended characters, which
adds up to the full 256 possible 8-bit characters.

Hope this isn't too long! It's been great to discuss this with someone else
who finds all of this interesting enough to put thought and effort into.
BTW, if you want to see my actual code I use, I'll gladly pull the relevant
sections out of my .php files for you.

Regards,
Ian





Hi,

I was quite interested by the password strength meter on your website and I was just curious as to how you chose the scale for the password strengths (i.e., "Weak", "Reasonable", etc.).  Is this a scale that you derived on your own or are you drawing from the computer security community in some way?  Thanks in advance for your time.

Sincerely,
Shing-hon




I searched for what was a common consensus of number of bits of entropy and how secure it was viewed with regard to the computer security community.  Lots of Google searches, several FAQs, and many mailing lists were read.

So, I basically made my own scale from what I was reading.
