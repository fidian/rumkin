<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:openSearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:batch="http://schemas.google.com/gdata/batch" xmlns:gAcl="http://schemas.google.com/acl/2007" xmlns:gs="http://schemas.google.com/spreadsheets/2006" xmlns:gd="http://schemas.google.com/g/2005" xmlns:thr="http://purl.org/syndication/thread/1.0" xmlns:sites="http://schemas.google.com/sites/2008" xmlns:dc="http://purl.org/dc/terms"><id>http://sites.google.com/feeds/content/site/minimifidianism</id><updated>2015-07-22T19:10:31.047Z</updated><title>Posts of Problems Only Tyler Has</title><link rel="alternate" type="text/html" href="http://sites.google.com/feeds/content/site/minimifidianism" /><link rel="http://schemas.google.com/g/2005#feed" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism" /><link rel="http://schemas.google.com/g/2005#post" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism" /><link rel="http://schemas.google.com/g/2005#batch" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/batch" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism?parent=1715856284860793463&amp;kind=announcement" /><generator version="1" uri="http://sites.google.com">Google Sites</generator><openSearch:startIndex>1</openSearch:startIndex><entry gd:etag="&quot;YDQpeyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/6627131446259482442</id><published>2012-03-12T23:28:16.261Z</published><updated>2013-04-12T18:47:54.819Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2013-04-12T18:46:33.259Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>Disabling Hyperthreading</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr">I never thought I would find what I feel is a really bad problem with the Linux scheduler, but it's hard to argue with my results.  I have an Acer Aspire One netbook and it has an 1.5 Ghz Intel Atom N550 inside.  It is a dual-core CPU with hyperthreading enabled.<br />
<br />At first I thought I was crazy or that something was fundamentally broken with my recent Ubuntu install on this fine machine.  I had been used to an HP Mini 110.  It's a dual core 1 Ghz AMD processor, and I expected better performance from this one.  Instead, I had found that my programs seemed to frequently hang, really crawl slowly, or sporadically operate well.  Very odd behavior.  I found, through use of my Mad Google Skillz, that it could be due to hyperthreading on the processor.  You see, hyperthreading isn't a real processing thread.  It's more like sharing parts of the same processing unit.  While one is doing an addition, another could use the unused multiplication routine.  If they both want to use bits of the CPU that overlap, then one process just has to wait.  In my case, that starved process waited and waited and waited.  It looked like Linux thought the hyperthreading was another core and treated it as though it could safely and quickly run threads on any of the available cores.  Thus, lots of jobs were running on the first core and few were running on the seconds.  The ones sharing a real core all got stalled.<br />
<h2><a name="TOC-Manually-Disabling-Hyperthreading" />Manually Disabling Hyperthreading</h2>
I found that the Ubuntu kernel, as well as RedHat and others, compile in an option to disable use of a CPU on the fly.  Fantastic!  Running two commands as root will kill the hyperthreading on my machine.<div><br />


<div class="sites-codeblock sites-codesnippet-block">

<code>echo 0 &gt; /sys/devices/system/node/node0/cpu1/online</code><br />
<code>echo 0 &gt; /sys/devices/system/node/node0/cpu3/online</code></div>

<br />Your CPU numbers may not match mine, so I don't suggest you use the above.<br />
<br />Now you might be wondering how you can get this to happen automatically on machines when they start up.  You can edit /etc/rc.local and add those two lines above the 'exit' line, but if the machine changes and you now don't have hyperthreading, then maybe you disabled two processors in your quad core machine.  Yikes!  Since programs are supposed to detect things like this and do work for you, I scoured the internet and tried to find a way to detect if a CPU is a hyperthreading CPU or not.  I didn't come up with anything at all.<br />
<br />But that didn't stop me.<br />
<h2><a name="TOC-Automatically-Disabling-Hyperthreading" />Automatically Disabling Hyperthreading</h2>I wrote this code to detect and disable all hyperthreading that looks like processors.  You can use this on any machine, whether or not it has hyperthreading and no matter how many processors and cores it actually has.  Let me know if this works well for you or what could be changed to make everyone happier.</div><div><br /></div><div>Update 2013-03-22:  Linux 2.6.x uses a comma as a separator, so changed <code>cut -d '-' -f 1</code> to a sed command.</div><div><br />
<div class="sites-codeblock sites-codesnippet-block">
<code>#!/bin/bash</code><br />
<br />
<code>
# Be careful to not skip the space at the beginning nor the end</code><br />
<code>
CPUS_TO_SKIP=" $(cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | sed 's/[^0-9].*//' | sort | uniq | tr "\r\n" "  ") "</code><br />
<br />
<br />
<code>
for CPU_PATH in /sys/devices/system/cpu/cpu[0-9]*; do</code><br />
<code>
    CPU="$(echo $CPU_PATH | tr -cd "0-9")"</code><br />
<code>
    echo "$CPUS_TO_SKIP" | grep " $CPU " &gt; /dev/null</code><br />
<code>
    if [ $? -ne 0 ]; then</code><br />
<code>
        echo 0 &gt; $CPU_PATH/online</code><br />
<code>
    fi</code><br />
<code>
done</code>
</div>
<br />With the above script saved safely on my hard drive and /etc/rc.local running this shell command, I automatically disable hyperthreading just after boot... until my machine gets cloned to another netbook that doesn't have hyperthreading, and then no CPUs are disabled.  The best of both worlds.<br /></div></div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/disabling-hyperthreading" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/6627131446259482442" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/6627131446259482442" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/6627131446259482442" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>disabling-hyperthreading</sites:pageName><sites:revision>8</sites:revision></entry><entry gd:etag="&quot;YD4peyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/2369961781886126565</id><published>2012-10-02T20:46:48.565Z</published><updated>2012-10-02T21:48:12.949Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2012-10-02T21:48:11.367Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>IE8 &lt;div&gt; Height Changing</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr">This was a problem that stumped me for quite some time.  I'm working to create a pagination plugin where you have a single parent <code>&lt;div class="results"&gt;</code> that contains several tile <code>&lt;div class="tile"&gt;</code> elements.  Basically, the structure looks a little like this:<div><br /></div><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>&lt;div class="results" style="overflow: hidden; position: relative"&gt;</code></div><div><span><code>    &lt;div class="resultsWrapper"&gt;</code></span><br /></div><div><span><span><code>    </code></span><code>    &lt;div class="tile"&gt;Result # 1&lt;/div&gt;</code></span><br /></div><div><span><span><code>    </code></span><code>    &lt;div class="tile"&gt;Result # 2&lt;/div&gt;</code></span><br /></div><div><span><span><span><code>    </code></span><code>    ...</code></span><br /></span></div><div><span><span><span><span><code>    </code></span><code>    &lt;div class="tile"&gt;Result # 100&lt;/div&gt;</code></span><br /></span></span></div><div><span><span><span><span><code>    &lt;/div&gt;</code></span><br /></span></span></span></div><div><span><span><span><code>&lt;/div&gt;</code></span></span></span></div></div><div><span><span><span><br /></span></span></span></div><div>I add some styles to <code>div.results</code> to make it only show a few tiles at a time.  Because the tiles can have a variable height, I use jQuery to calculate this:</div><div><br /></div><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>// Error detection and bounds checking removed for clarity</code></div><div><code>var page = 3;  // zero-based indexing</code></div><div><code>var perPage = 5;</code></div><div><code>var children = $('div.results').children().children();  // Get the tiles</code></div><div><code>var firstChildTop = Math.floor(children.get(0).position().top);</code></div><div><code>var firstVisibleTop = Math.floor(children.get(page * perPage).position().top);</code></div><div><code>var lastVisibleBottom = Math.floor(children.get((page + 1) * perPage).position().top);</code></div><div><code>// Show the divs on this page</code></div><div><code>$('div.results').animate({ height: lastVisibleBottom - firstVisibleTop });</code></div><div><code>$('div.results').children().animate({ marginTop: firstVisibleTop - firstChildTop });</code></div></div><div><br /></div><div>Remember, this is just an example to help illustrate what I am trying to do.  You'll need quite a bit more code to make a working pager plugin for jQuery.  Anyway, so this will appear to the browser that there's a sliding series of <code>div.tile</code> elements moving to the "page" that you are on.  With the "<code>overflow: hidden</code>" and the negative margin, this acts like a little window seeing just a portion of the larger <code>div.resultsWrapper</code> that is sliding around to show just what we need.</div><div><br /></div><div>Except in IE8.  It's also not the case sometimes in IE9 when rendering in IE8 mode, but only sometimes.</div><div><br /></div><div>The problem boils down to the heights of the elements.  When IE8 slides the <code>div.resultsWrapper</code> up, the <code>div.tile</code> elements forget their heights.  It's crazy, but you could have some JavaScript like this to show the heights:</div><div><br /></div><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>var h = 'Heights: ';</code></div><div><code>$('div.tile').each(function () {</code></div><div><span><code>    h += ' ' + $(this).height();</code></span><br /></div><div><span><code>});</code></span></div><div><span><code>console.log(h);</code></span></div></div><div><span><br /></span></div><div><span>You'll see output like this when at the top of the list:</span></div><div><span><br /></span></div><div><div class="sites-codeblock sites-codesnippet-block"><code>Heights:  212 197 197 202 212 207 ...</code></div></div><div><br /></div><div>Now use a little jQuery magic to scroll down by setting a negative margin-top CSS property on <code>div.resultsWrapper</code>.  Let's say you scrolled down so just a little of the bottom of the fourth element is shown.  Move your mouse over the <code>div.results</code> element.  Now, run that JavaScript again that shows the heights.  I was seeing this:</div><div><br /></div><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>Heights:  47 47 47 768 212 207 ...</code></div><div /></div><br /><div>The height of the first three shrunk to just the padding I had on <code>div.tile</code> and the fourth tile strangely sucked up most (but not exactly all) of the height that was missing.  You can move back to the top and the content is messed up until you mouse over <code>div.results</code>.  I set a global breakpoint and no JavaScript runs when I mouse over <code>div.results</code>, yet that's still when the heights changed.  After much trial and error, I found that the contents of the tiles were to blame.  Here's closer to what my tiles looked like, and I bet you'll start to get a feel for where the problem lies.</div><div><br /></div><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>&lt;div class="tile"&gt;</code></div><div><span><code>    &lt;div class="productImage" style="float: left"&gt;&lt;img src="..."&gt;&lt;/div&gt;</code></span><br /></div><div><span><span><code>    &lt;div class="productDescription" style="float: left"&gt;This is result #1&lt;/div&gt;</code></span></span></div><div><span><span><span><code>    &lt;div class="clear" style="clear: both"&gt;&lt;/div&gt;</code></span><br /></span></span></div><div><span><span><code>&lt;/div&gt;</code></span></span></div><div /></div><span><span><br /></span></span><div><span><span>My divs used "<code>float: left</code>" to position them inside the </span></span><code>div.tile</code><span><span> element properly.  This works well in all browsers and looks great even in IE8 and IE7 (I have no need to go lower).  The only browser that chokes is IE8.  It must do something when the <code>div.tile</code> elements are above the visible area and it just doesn't keep them loaded or positioned properly.  This feels a lot like another type of "peekaboo bug" that has plagued IE with floats ever since they were introduced in that browser.</span></span></div><div><span><span><br /></span></span></div><div><span><span>The fix:  Do not use float.  Yep, I tried several variations, but nothing ever worked with dynamically sizing content and floats.  In the end "<code>float: left</code>" was replaced with "<code>display: inline-block</code>" and it again looks perfect in all browsers.</span></span></div></div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/ie8divheightchanging" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/2369961781886126565" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/2369961781886126565" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/2369961781886126565" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>ie8divheightchanging</sites:pageName><sites:revision>2</sites:revision></entry><entry gd:etag="&quot;YD0peyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/2096649146019846376</id><published>2012-09-25T22:13:59.464Z</published><updated>2012-09-25T22:13:59.468Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2012-09-25T22:13:59.460Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>WCF and gzip compression</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr"><div>I was helping to diagnose a problem where web requests to a service were being troublesome.  It always enabled compression on the output stream, whether or not the client asked for it.  Normally that is not a problem.  We were using PHP to make SOAP calls and tied that to PHP's curl library because we had some special requirements regarding request and response headers that were necessary.</div><div><br /></div><div>PHP's SOAP library (when fetching via the curl module) was saying that there was no response or that there were problems decompressing the stream.  Wget did not work.  The curl command-line tool worked.  Using a sniffer on the network showed me that data was coming across the wire.  When that data was written to disk, gzip would not decompress it but zcat would.</div><div><br /></div><div>Everything worked like a charm when compression was disabled, but it was absolutely necessary that the compression was enabled and forced on in our production environment.</div><div><br /></div><div>We more carefully analyzed the responses from the server and found that there was random-ish looking data (as is expected) for most of the response and then perhaps about 1/3 is NULL bytes or (even worse) XML from some sort of SOAP request.  It looks like we're leaking memory contents.  Very undesirable.</div><div><br /></div><div>We obtained the source code at about the time that I noticed all response lengths were powers of 2:  256 bytes, 512 bytes, 1k, 2k, 4k, 8k.  We're sending back some sort of buffer that was allocated.  Here's the code that was affected -- you may notice it looks a lot like many other copies of this code on the web.</div><div><br /></div><div><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>//Helper method to compress an array of bytes</code></div><div><code>static ArraySegment&lt;byte&gt; CompressBuffer(ArraySegment&lt;byte&gt; buffer, BufferManager bufferManager, int messageOffset)</code></div><div><code>{</code></div><div><span style="white-space:pre">	</span><code>MemoryStream memoryStream = new MemoryStream();</code></div><div><span style="white-space:pre">	</span><code>memoryStream.Write(buffer.Array, 0, messageOffset);</code></div><div><br /></div><div><span style="white-space:pre">	</span><code>using (GZipStream gzStream = new GZipStream(memoryStream, CompressionMode.Compress, true))</code></div><div><span style="white-space:pre">	</span><code>{</code></div><div><span style="white-space:pre">		</span><code>gzStream.Write(buffer.Array, messageOffset, buffer.Count);</code></div><div><span style="white-space:pre">	</span><code>}</code></div><div><br /></div><div><br /></div><div><span style="white-space:pre">	</span><code>byte[] compressedBytes = memoryStream.ToArray();</code></div><div><span style="white-space:pre">	</span><code>byte[] bufferedBytes = bufferManager.TakeBuffer(compressedBytes.Length);</code></div><div><br /></div><div><span style="white-space:pre">	</span><code>Array.Copy(compressedBytes, 0, bufferedBytes, 0, compressedBytes.Length);</code></div><div><br /></div><div><span style="white-space:pre">	</span><code>bufferManager.ReturnBuffer(buffer.Array);</code></div><div><span style="white-space:pre">	</span><code>ArraySegment&lt;byte&gt; byteArray = new ArraySegment&lt;byte&gt;(bufferedBytes, messageOffset, bufferedBytes.Length - messageOffset);</code></div><div><span style="white-space:pre">	</span><code>return  byteArray;</code></div><div><code>}</code></div></div></div><div><br /></div>This actually comes from one version of an example that Microsoft produced.  In our case we thought it was Iconic.Zlib but the above code uses System.IO.Compression.GZipStream, so it isn't related to the compression library.  That works like a charm.  What's broken about this code is the byteArray and how many bytes are copied to it.  That last line should instead look like this:<div><br /></div><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>ArraySegment&lt;byte&gt; byteArray = new ArraySegment&lt;byte&gt;(bufferedBytes, messageOffset, </code><b><code>compressedBytes.Length</code></b><code>);</code></div><div /></div><br /><div>Once you make this change, your HTTP responses should no longer be exactly equal to powers of 2.  You can double-check this by looking for the Content-Length headers when you sniff the traffic or use some tool that will show you the full response headers.</div><div><br /></div><div>I hope that others can spread this good knowledge out to the various other forums for when people have problems with this.  I believe that this is the reason that Chrome has issues with compressed data when people are doing things like this.  I found forum postings mentioning that Chrome is extra picky about compressed data and how compressed data from some C# services were not working in Chrome.</div></div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/wcfandgzipcompression" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/2096649146019846376" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/2096649146019846376" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/2096649146019846376" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>wcfandgzipcompression</sites:pageName><sites:revision>1</sites:revision></entry><entry gd:etag="&quot;YD4peyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/5442027769991125524</id><published>2012-06-18T13:10:37.036Z</published><updated>2012-06-18T13:13:12.022Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2012-06-18T13:13:11.422Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>Chef Upgrade Issue</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr">Once again, I had a problem with Opscode Chef, but for a very understandable reason.  First, while trying to spin up an instance, I see messages like this at the end.<div><br /></div><div><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>ERROR: Server returned error for http://ec2-50-17-230-193.compute-1.amazonaws.com:4000/cookbooks/phpunit/0.9.1/files/1ac61a28fa057aeb34ca4e5071e9c96c, retrying 2/5 in 8s</code></div><div><code>ERROR: Server returned error for http://ec2-50-17-230-193.compute-1.amazonaws.com:4000/cookbooks/phpunit/0.9.1/files/1ac61a28fa057aeb34ca4e5071e9c96c, retrying 3/5 in 16s</code></div><div><code>ERROR: Server returned error for http://ec2-50-17-230-193.compute-1.amazonaws.com:4000/cookbooks/phpunit/0.9.1/files/1ac61a28fa057aeb34ca4e5071e9c96c, retrying 4/5 in 29s</code></div><div><code>ERROR: Server returned error for http://ec2-50-17-230-193.compute-1.amazonaws.com:4000/cookbooks/phpunit/0.9.1/files/1ac61a28fa057aeb34ca4e5071e9c96c, retrying 5/5 in 53s</code></div><div><code>ERROR: Running exception handlers</code></div><div><code>FATAL: Saving node information to /var/chef/cache/failed-run-data.json</code></div><div><code>ERROR: Exception handlers complete</code></div><div><code>FATAL: Stacktrace dumped to /var/chef/cache/chef-stacktrace.out</code></div><div><code>FATAL: Net::HTTPFatalError: 500 "Internal Server Error"</code></div></div></div><div><br /></div><div>I then peeked into <b style="font-family:Times New Roman;font-size:medium;font-weight:normal"><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap">/var/log/chef/server.log and saw some messages.  Not quite helpful, but perhaps a clue?</span></b></div><div><b style="font-family:Times New Roman;font-size:medium;font-weight:normal"><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><br /></span></b></div><div><b style="font-family:Times New Roman;font-size:medium;font-weight:normal"><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><div /><div class="sites-codeblock sites-codesnippet-block"><div><code>merb : chef-server (api) : worker (port 4000) ~ Params: {"cookbook_version"=&gt;"1.0.0", "action"=&gt;"show_file", "cookbook_name"=&gt;"apache2", "checksum"=&gt;"ab1792e9de7461ddf4861e461c0c8a24", "controller"=&gt;"cookbooks"}</code></div><div><code>merb : chef-server (api) : worker (port 4000) ~ undefined method `file_location' for # - (NoMethodError)</code></div></div><div><br /></div><div>The file_location was set correctly, so now I am stumped.  I restarted chef server, rebuilt the database as I mentioned on a <a href="http://www.fidian.com/problems-only-tyler-has/opscode-chef-rabbitmq">previous blog post</a> and uploaded everything again to the server.  No luck with any of those.  The failure point wasn't always on the same package.  It seemed to hop around to different packages at different times.</div><div><br /></div><div>So, now I check versions of the packages that are installed.</div><div><br /></div><div><b style="font-family:Times New Roman;white-space:normal;font-size:medium;font-weight:normal"><div class="sites-codeblock sites-codesnippet-block"><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><code>19:32 utilities:/tmp$ sudo dpkg -l | grep chef</code></span><br /><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><code>ii  chef                              0.10.8-2                                   A systems integration framework, built to br</code></span><br /><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><code>ii  chef-expander                     0.10.4-1                                   A systems integration framework, built to br</code></span><br /><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><code>ii  chef-server                       0.10.4-1                                   A meta-gem to install all server components</code></span><br /><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><code>ii  chef-server-api                   0.10.4-1                                   A systems integration framework, built to br</code></span><br /><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><code>ii  chef-server-webui                 0.10.4-1                                   A systems integration framework, built to br</code></span><br /><span style="font-size:15px;font-family:Arial;background-color:transparent;vertical-align:baseline;white-space:pre-wrap"><code>ii  chef-solr                         0.10.4-1                                   Manages search indexes of Chef node attribut</code></span></div></b></div><div><br /></div><div>You'll see that one package is at 0.10.8 and the rest are all 0.10.4.  Could that be it?  Reinstalling the chef package didn't force upgrades of the others, so I just manually used <code>apt-get</code> to upgrade the other chef packages and it started to work again.</div><div><br /></div><div>Problem solved.</div></span></b></div></div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/chefupgradeissue" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/5442027769991125524" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/5442027769991125524" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/5442027769991125524" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>chefupgradeissue</sites:pageName><sites:revision>2</sites:revision></entry><entry gd:etag="&quot;YD0peyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/211013441406828679</id><published>2012-06-17T23:40:27.029Z</published><updated>2012-06-17T23:40:27.037Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2012-06-17T23:40:27.020Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>Backups and Recovery</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr">Years ago, I had the task to create a very large network share.  I decided to build a Linux box with 6 raided 1.5 TB drives.  At the time, it was a hefty cost.  So, when we were planning this whole thing out, it was decided that there would really be no possibility of a backup since getting tapes and building a secondary machine were both cost prohibitive.  Yes, it was a risk, but one that was acceptable. To counter the <strike>chance</strike> inevitability of a future failure, we decided to use RAID 5 and software RAID on Linux.  That way, as long as we could get five of the six drives up and could build/borrow another Linux box, we should be able to recover the data.  So, no actual backup strategy but at least some fault tolerance was built into the design.  I also reasoned that I could get the data off with a minor amount of additional hardware purchased at the time of failure.<div><br /></div><div>And yesterday the machine failed.</div><div><br /></div><div>Now, I don't have another Linux box with six SATA ports on it, so I made a trip to Microcenter and purchased some handy SATA to USB devices in order to get five drives running.  That way I could run in degraded mode and mount the filesystem as read-only so I could get the data off the drives.  I discovered that one of the things I picked was actually IDE to USB, and so I made trip #2 to Microcenter.  After that, I was wiring things together and one of the enclosures failed to work.  Trip #3.  At least they're really nice at the returns counter.</div><div><br /></div><div>I plug in the drives into a USB hub, then plug in the hub and additional destination drives to my laptop.  I'm recovering at a mere 20 MB/s, so it will take a long time, but at least I didn't have the drives full when I started.</div><div><br /></div><div>So, here I am, pondering the things that went well and the things that were terrible about this strategy, and I have to say that I am quite pleased with how everything is panning out.  I figure that I should give you an overview of the various pieces that were considered during building the system and how well things worked for me during this time of failure.  It might keep my mind off the fact that I'm now recovering my RAID on a hodgepodge of cabling, I've got my kids looking at the flashing lights, I'm pretty sure one of the enclosures has touchy wiring making it motion sensitive and there's a thunderstorm coming.  I wish I plugged all this into a UPS before I started.</div><h2><a name="TOC-Plan-for-Failure---Backups-and-Recovery" />Plan for Failure - Backups and Recovery</h2><div>I knew that I'd be building a custom system that had more space on it than what was on all of the servers, NAS devices and desktops (combined) at my current place of business.  When this would fail, how would I get data off the machine?  Have a backup plan.  Mine was really to get the information again through a very long and painful process because I could not afford to double my costs.</div><div><br /></div><div>To mitigate the chance of loss, I did decide that I'd always be able to afford one more drive to be used by the RAID for the "R" part (redundant).  I'd need at least two drives to fail for me to lose the data.</div><h2><a name="TOC-Stagger-Hard-Drive-Purchases" />Stagger Hard Drive Purchases</h2><div>When you purchase the drives for your devices, you want to get them from different batches.  This is because hard drives manufactured at the same time tend to break at about the same time.  I didn't do this either due to time constraints, but you should do what you can.</div><h2><a name="TOC-When-It-Fails-What-Then-" />When It Fails, What Then?</h2><div>Alerts were set up to monitor the drives and let me know immediately if the data was at risk.  I'd just go out and buy a new hard drive and add it to the RAID to recover.  Not a big deal... as long as the other five drives stayed running.</div><div><br /></div><div>If my machine died, part of the recovery plan was to go out and purchase USB adapters for the drives.  At the time, those were a little expensive and they came down greatly in price.  I figured that perhaps USB 3 could be everywhere when there was a drive failure, so I could get improved recovery speeds.</div><h2><a name="TOC-Avoid-Proprietary-Lock-In" />Avoid Proprietary Lock-In</h2><div>One big thing to avoid is setting up a hardware-based RAID array.  Yes, they offload the RAID work to some other device, but benchmarks show that it isn't very expensive computationally to use a software based RAID.  Another advantage of using a software RAID is that you can use multiple channels on the board to fetch and store information instead of passing everything through a single controller.  Lastly, you avoid proprietary RAID formats.  This last topic is a huge hurdle.</div><div><br /></div><div>When you use a hardware RAID card, I strongly suggest you buy no fewer than two at the exact same time and confirm that they have the same firmware on them.  I've experienced and heard of people having issues recovering a RAID when they use newer cards, different models and even with minor firmware changes.  If your one controller dies, you will need a backup controller that can get the data off the RAID, otherwise you've got a lot of useless disks.</div><div><br /></div><div>Now, compare these problems to software RAID.  If I keep a CD of the distribution I used to make the RAID, I'll be able to install it again and recover.  Plus, it is usually forward compatible with future versions of that software.  Years ago I used mdadm to set up the RAID and today I used the current mdadm version to recover the data from the drives.  No hassle at all.</div><h2><a name="TOC-Power-Problems" />Power Problems</h2><div>Since you are investing all this time and energy in making a bulletproof system, you probably want to put it on a UPS to help your hardware last longer.  The local power grid goes through brownouts, power outages, spikes and has lots of noise from adjacent buildings, blenders, fluorescent lights and other computers.  A UPS stops that and conditions the power so your hardware doesn't get beaten up nearly as much.  I have a feeling that something like that fried the big computer so that it can only stay on for two minutes at a time, which is why I'm trying to recover this data with my laptop.</div><h2><a name="TOC-Test-Your-Backup-Plan" />Test Your Backup Plan</h2><div>I've worked at places where the backup job appeared to be running for months, but never actually wrote data to the disk.  We were able to recover some of the data painfully (RAID failure there as well), but it also taught us to try to restore files from our backups every now and then.  Acrobats test that their net will hold their weight before they blindly trust their lives.  Your data is depending on you; test your "safety net" backups before you rely on them.</div><h2><a name="TOC-Summary" />Summary</h2><div>Keep an eye on the current safety of your systems.  Set up monitoring to ensure the health of your system is consistently good.  Backups are good, redundancy is good.  Plan for failure and test your failure plans when you can.</div><div><br /></div><div>Thankfully my drives were not full, otherwise I'd be spending abut 110 hours recovering them.  As it is, I only have perhaps another 12 hours.  The hardest part is that I'm juggling data to drives that are significantly smaller, but I would much rather have my data than try to regenerate it again!</div></div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/backupsandrecovery" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/211013441406828679" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/211013441406828679" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/211013441406828679" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>backupsandrecovery</sites:pageName><sites:revision>1</sites:revision></entry><entry gd:etag="&quot;YD8peyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/3242174966935147444</id><published>2012-02-17T13:17:04.332Z</published><updated>2012-06-08T17:24:04.250Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2012-06-08T17:24:03.130Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>Using "GRANT ALL" With Amazon's MySQL RDS</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr">I was recently put in charge of migrating some data to Amazon's RDS.  This whole cloud thing has got some serious potential, and I like how Amazon has set things up.   However, when I tried to grant all privileges to my user ...<br /><br />
<div class="sites-codeblock sites-codesnippet-block"><code>mysql&gt; GRANT ALL ON *.* TO fidian@`%`;</code><br />
<code>ERROR 1045 (28000): Access denied for user 'masterAccount'@'%' (using password: YES)</code><br />
<code>mysql&gt; GRANT ALL PRIVILEGES ON *.* TO fidian@`%`;</code><br />
<code>ERROR 1045 (28000): Access denied for user 'masterAccount'@'%' (using password: YES)</code></div><div><br /></div>
"That's odd," I thought, and I tried again with a slight variation.<br /><br />
<div class="sites-codeblock sites-codesnippet-block"><code>mysql&gt; GRANT ALL ON *.* TO myUser@`%`;</code><br />
<code>ERROR 1045 (28000): Access denied for user 'masterAccount'@'%' (using password: YES)</code></div><div><br /></div>
Interesting.  I can grant all privileges to a database, but not to all databases?  Well, there needs to be a way to do it because I will be creating and removing testing databases on the fly and I will need access to create and remove these test databases without logging in as the master account.<br /><br />
After much Googling and reading of the MySQL <a href="http://dev.mysql.com/doc/refman/5.5/en/grant.html">grant syntax</a>, I assured myself that I'm not totally crazy and that Amazon just has their RDS instance locked down to prevent me from going in and mucking things up.  I also read this tiny gem:<br /><br />
<blockquote style="margin:0 0 0 40px;border:none;padding:0px">
The “_” and “%” wildcards are permitted when specifying database names in GRANT statements that grant privileges at the global or database levels.<br /><br />
</blockquote>
Fantastic!  Let's put this into action:<br /><br />
<div class="sites-codeblock sites-codesnippet-block"><code>mysql&gt; GRANT ALL ON `%`.* TO fidian@`%`;</code><br />
<code>Query OK, 0 rows affected (0.00 sec)</code></div><div><br /></div>
Now I can also create more users, such as "frank" and then "<code>GRANT ALL ON `frank\_%`.* to frank@`%`;</code>" in order to allow this user full access to just his tables.<br /><br />
I always like knowing the root cause of why problems exist, so I felt I should be able to explain why * fails and `%` succeeds.  It turns out that your master account doesn't have all privileges on the database.  It has most privileges (not FILE, SUPER, REPLICATION_SLAVE nor CREATE_TABLESPACE).  I'm not positive, but I believe that since the master account doesn't have SUPER, the mysql.* tables are considered off-limits.  Since I don't have access to the mysql.* tables, I can't grant permissions on *.* since that would match mysql, and `%`.* appears to not match those system tables.<br /><br />
So, the moral of the story is to use `%`.* instead of *.*.  And, come to think of it, not granting permissions to the mysql table for all of your users is probably a good thing anyway.</div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/using-grant-all-with-amazons-mysql-rds" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/3242174966935147444" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/3242174966935147444" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/3242174966935147444" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>using-grant-all-with-amazons-mysql-rds</sites:pageName><sites:revision>3</sites:revision></entry><entry gd:etag="&quot;YDkpeyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/6071642006993989149</id><published>2012-03-21T02:25:24.908Z</published><updated>2012-06-08T17:23:22.913Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2012-06-08T17:23:21.955Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>Renaming Windows Network Adapter for VirtualBox</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr">The problem with VirtualBox is that on Mac it names its virtual host-only adapters "vboxnet0" and the like.  On Windows they are called "VirtualBox Host-Only Ethernet Adapter", maybe with "#2" added at the end.  Normally this really is not a problem, but it is if you are working in a Macintosh-dominated environment and they have been using <a href="http://www.opscode.com/chef/">Opscode Chef</a>,  <a href="http://vagrantup.com/">Vagrant</a> and <a href="https://www.virtualbox.org/">VirtualBox</a> to bundle up development environments into boxes.  These virtual machines may be scripted to enable specific networking configurations, such as making a host-only virtual ethernet adapter available to the virtual machine so your VMs can easily network to just each other.  The problem is that the default host-only adapter name changes based on your OS, so now my configuration that's stored in the box from Vagrant is expecting an adapter named "vboxnet0" and mine isn't called that at all.  Starting the VM in VirtualBox will cause problems and then Vagrant will think the install failed.<br /><br />
You'd think it would be as easy as just going to the network settings in the Windows control panel and then right-clicking the adapter and hitting "Rename".  No, it's unfortunately not nearly that simple.<br />
<h2><a name="TOC-Contributing-Factors" />Contributing Factors</h2>
This is a slightly more painful process because of the following wrinkles:<br />
<ol><li>This problem is a little more complex because this virtual ethernet adapter can be created automatically by Vagrant.  It's really a nice setup on the Mac, but because of the weird naming it can be irritating on Windows.</li>
<li>When Vagrant kills a machine, it can optionally clean up network adapters that are no longer used.  This means you could lose your network adapter even if you do manage to call it "vboxnet0" on your Windows machine.</li>
<li>The box files that Vagrant produces has the machine settings saved in them.  This includes the amount of RAM, the hard drive image files (see <a href="http://www.fidian.com/programming/shrinking-vm-disk-images">Shrinking VM Disk Images</a> if you want yours smaller), and the networking configuration.  Vagrant can download the box during installation, so I don't want a manual step for modifying this box file before installation.</li>
<li>The standard advice I'd find on VirtualBox forums and other places would be to always manually go in and check / change the network settings.  My goal is a fully automated solution, which means everything get scripted.</li>
<li>I could use different configurations and double the amount of images I'd need to maintain in order to support Windows, but that's just making a problem bigger and more unmanageable.</li>
<li>Windows XP's security model for registry changes is different than Windows 7.  More on this later.</li>
</ol>
Most of the problems can be eliminated by being able to rename a network adapter right after we create one with Vagrant.  It could be added safely to the scripts that spin up machines and people from around the world will rejoice.  Well, at least a couple might hum in a happy way.<br />
<h2><a name="TOC-The-Solution" />The Solution</h2>
It turns out that the name of the network adapter, as seen by VirtualBox, is secreted away in the registry.  If you use regedit to check out HKLM\SYSTEM\CurrentControlSet\Enum\Root\NET and pick one of the keys listed there.  If you check out the Service value and it says "VBoxNetAdp", then you are in luck.  If there is a FriendlyName value just change it to "vboxnet0".  If not, make a FriendlyName value and set it to "vboxnet0".  Reboot or restart all of your VirtualBox software and you should now see this renamed network adapter.<br /><br />
Unfortunately, this is where we hit a snag.  On Windows XP you may need administrator privileges to set this value.  On Windows 7 you need to use the "SYSTEM" account (not the administrator account) or else you will get the wrath of the "access denied" alert.  Don't fret, I've got you covered.<br />
<h2><a name="TOC-Manual-Process" />Manual Process</h2>
<ol><li>Run VirtualBox and make a virtual host-only network adapter</li>
<li>Tie this virtual adapter to a new virtual machine</li>
<ul><li>This is an optional step and is useful so Vagrant doesn't delete the network adapter</li></ul>
<li>Run regedit as administrator</li>
<li>Browse to HKLM\SYSTEM\CurrentControlSet\Enum\Root\NET</li>
<ul><li>This could be a specific control set if you are administering another person's account</li></ul>
<li>Look at the keys under here for one with a System value of "VBoxNetAdp"</li>
<li>If there is not a FriendlyName value, right-click in the right pane and attempt to add a new string value; otherwise double-click the FriendlyName value and rename the adapter to "vboxnet0"</li>
<ul><li>If you get an "access denied" message, grant Administrator permission to modify the key by right-clicking on the key, select Permissions -&gt; Advanced -&gt; Owner and grant full control to Administrators.  Apply and try to add or change the value again in regedit.</li></ul>
<li>"VBoxManage list hostonlyifs" from the command line should now list your new value.  If not, double-check that the FriendlyName is properly set.  Then try rebooting the machine.</li>
</ol>
Fantastic.  It's now named vboxnet0.  You could use this to rename the network adapter to anything you like if vboxnet0 doesn't tickle your fancy.<br />
<h2><a name="TOC-Automatic-Process" />Automatic Process</h2>
If you are in a situation like where I was and you need to get this deployed to many machines, you will want to write a little script.  There's two key parts to the script - scanning and escalating.  The scanning part is pretty straightforward.  This is not real code, just in case you were wondering.<br /><br />
<div class="sites-codeblock sites-codesnippet-block"><code>f</code><code>or each key in HKLM\SYSTEM\CurrentControlSet\Enum\Root\NET as key<br />
<span>    if value of (key + "\System") == "VBoxNetAdp"<br />
</span><span>    <span>    if value of (key + "\FriendlyName") == "vboxnet0"<br />
<span>    <span>    <span>    return SUCCESS // It's already there<br />
</span></span></span><span>    <span>    end<br />
</span></span><span>    end<br />
next<br />
<br />
</span></span></span><code>f</code><code>or each key in HKLM\SYSTEM\CurrentControlSet\Enum\Root\NET as key<br />
    if value of (key + "\System") == "VBoxNetAdp"<br />
<span>    <span>    if can set value of (key + "\FriendlyName") to "vboxnet0" then<br />
<span>    <span>    <span>    return SUCCESS // I made one exist<br />
<span>    <span>    else<br />
</span></span><span>    <span>    <span>    return FAILURE // Could not rename - maybe escalate?<br />
</span></span></span><span>    <span>    end<br />
<span>    end<br />
end<br />
<br />
return FAILURE // None detected to change</span></span></span></span></span></span></span></span></code></code></div><div><br /></div>
What this will do is first scan all net adapters for a VirtualBox network adapter.  If it finds one with the name "vboxnet0" it will exit since we don't need to do any work.  Failing that, it will scan again to find the first VirtualBox network adapter and attempt to rename it to vboxnet0.  This will return either success or failure.  If no VirtualBox network adapters were found, this script fails.<br /><br />
Next up, the escalating of privileges.  Either you can write a real program or else you can perhaps use <a href="http://technet.microsoft.com/en-us/sysinternals/bb897553">PsExec</a> to grant you the right privileges when running a command-line tool.<br /><br />
<div class="sites-codeblock sites-codesnippet-block"><code>Attempt to rename as regular user</code><br />
<br />
<code>if rename_script_result == FAILURE</code><br />
<span><code>    Attempt to rename as Administrator</code><br />
<br />
<span><code>    if rename_script_result == FAILURE</code><br />
<span><code>    </code><span><code>    Attempt to rename as SYSTEM</code><br />
</span></span><br />
<span><code>    </code><span><code>    if rename_script_result == FAILURE</code><br />
<span><code>    </code><span><code>    </code><span><code>    return FAILURE // Could not do it</code><br />
<span><code>    </code><span><code>    end</code><br />
</span></span><span><code>    end</code><br />
</span><code>end</code></span></span></span></span></span></span></span></div><div><br /></div>
I once wrote some JavaScript to do this and executed it with cscript in Windows, though I believe this could be done better as an application that could prompt for Administrator privileges and properly drop down to SYSTEM instead of relying on PsExec trickery.  It also turns out that you can run "cmd" as Administrator, but people have a hard time running cscript as administrator in a command shell, and there is no way that I found to run a command-line tool as SYSTEM without PsExec.  I've tried my best to recreate that script from memory and attached it below.  I haven't tested it much, so I'd appreciate feedback if there's a shortcoming.<br /><br />
While trying to get my solution to work, I found perhaps a half dozen ways that UAC didn't work with regard to batch files and windows scripting host.  I guess that there were enough skript kiddiez out there using these tools that Microsoft needed to clamp down on the interaction between the shell and programs. I can't blame them, but it is sure hard to pop open a UAC prompt on Windows 7 from a command line; I certainly didn't find a good way.<br /></div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/renaming-windows-network-adapter" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/6071642006993989149" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/6071642006993989149" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/6071642006993989149" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>renaming-windows-network-adapter</sites:pageName><sites:revision>5</sites:revision></entry><entry gd:etag="&quot;YD8peyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/2977633266674986726</id><published>2012-02-25T17:53:52.214Z</published><updated>2012-06-08T17:22:23.835Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2012-06-08T17:22:22.971Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>Opscode Chef + RabbitMQ</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr">I have the privilege of working with Opscode Chef at work, maintaining the recipes as various projects move to "the cloud" or otherwise want to script the setup of their environments.  While spinning up many new machines with knife, very rarely I will hit this problem.  After trying pretty hard to find the root cause and why it happens on the internet, I'll sum up what I found on to this one page.  Maybe it will help solve this problem for you too?<br />
<h2><a name="TOC-Symptoms" />Symptoms</h2>
I was running <code>knife</code> and it wasn't doing anything at all.  No CPU activity and no interaction with anything else.  No error messages popped up.  No output to console at all after hitting enter to start the command. For all intents, it appeared that the program was hung.  I didn't run strace on it, but I did attempt to use <code>kill</code> and <code>kill -9</code> on the process to no effect.<br />
<br />I really hate when things like that happen.<br />
<br />At this point, the server started acting funny and I couldn't spin up any instances, so I rebooted.<br />
<h2><a name="TOC-Try-2" />Try #2</h2>
So I tried to spin up the server with <code>knife</code> again and got some interesting error messages:<div><br />
<div class="sites-codeblock sites-codesnippet-block"><code><b>INFO:</b> *** Chef 0.10.4 ***<br />
<b>INFO:</b> Client key /etc/chef/client.pem is not present - registering<br />
<b>INFO:</b> HTTP Request Returned 500 Internal Server Error: Connection refused - connect(2)<br />
<b>ERROR:</b> Server returned error for http://bluemoon.fuf.me:4000/clients, retrying 1/5 in 4s<br />
<b>INFO:</b> HTTP Request Returned 409 Conflict: Client already exists<br />
<b>INFO:</b> HTTP Request Returned 403 Forbidden: You are not allowed to take this action.<br />
<b>FATAL:</b> Stacktrace dumped to /var/chef/cache/chef-stacktrace.out<br />
<b>FATAL:</b> Net::HTTPServerException: 403 "Forbidden"</code></div>
<br />Uh-ho.  Why would the server say that the client already exists?  Let's go see what the logs say.  I browse <code>/var/log/chef/server.log</code> and find this one gem of a line, which I quoted below.  It was <i>not</i> at the bottom of the file; I needed to scroll up several screens of logs in order to see this problem.</div><div><br />
<div class="sites-codeblock sites-codesnippet-block"><code>merb : chef-server (api) : worker (port 4000) ~ Connection refused - connect(2) - (Bunny::ServerDownError)</code></div>
<br />What is this Bunny and why is it down?  Turn out that RabbitMQ will not start because it has corrupted databases.  Bummer.  You may ask "how can I fix such a thing?"  Well, you can't really repair the databases.  Instead, we just delete them.<br />
<h2><a name="TOC-The-Fix" />The Fix</h2>When you delete the RabbitMQ databases (conveniently located in <code>/var/lib/rabbitmq/mnesia</code>), you are not done yet.  In order to set up Chef in RabbitMQ, you need to add a vhost, username, password, and permissions.  The vhost is <code>/chef</code>, username is <code>chef</code>, and the user should have all permissions to the vhost.  The slightly tougher part is getting the password, but it's found in <code>/etc/chef/solr.rb</code> as <code>amqp_pass</code>.  Here is a shell script I used to fix the problem.  You're welcome to use it.</div><div><br />
<div class="sites-codeblock sites-codesnippet-block"><font color="#006000" face="monospace">#!/bin/bash<br />
</font><span style="color:rgb(0,96,0);font-family:monospace"><br />
# Fix RabbitMQ by removing the databases<br />
<br />
service rabbitmq-server stop<br />
</span><br />
<font color="#006000" face="monospace">if [ -d /var/lib/rabbitmq/mnesia ]; then<br />
</font><span style="color:rgb(0,96,0);font-family:monospace;white-space:pre">	</span><font color="#006000" face="monospace">echo Removing mnesia directory<br />
</font><span style="color:rgb(0,96,0);font-family:monospace;white-space:pre">	</span><span style="color:rgb(0,96,0);font-family:monospace">rm -r /var/lib/rabbitmq/mnesia -r<br />
</span><span style="color:rgb(0,96,0);font-family:monospace">fi<br />
</span><span style="color:rgb(0,96,0);font-family:monospace"><br />
service rabbitmq-server start<br />
<br />
<br />
# Add the Chef vhost, username, password, and permissions<br />
</span><span style="color:rgb(0,96,0);font-family:monospace"><br />
rabbitmqctl add_vhost /chef<br />
</span><span style="color:rgb(0,96,0);font-family:monospace">PASS=$( grep ^amqp_pass /etc/chef/solr.rb | cut -d '"' -f 2 );<br />
</span><span style="color:rgb(0,96,0);font-family:monospace">rabbitmqctl add_user chef $PASS<br />
</span><span style="color:rgb(0,96,0);font-family:monospace">rabbitmqctl set_permissions -p /chef chef ".*" ".*" ".*"</span></div></div></div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/opscode-chef-rabbitmq" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/2977633266674986726" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/2977633266674986726" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/2977633266674986726" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>opscode-chef-rabbitmq</sites:pageName><sites:revision>3</sites:revision></entry><entry gd:etag="&quot;YDgpeyY.&quot;"><id>http://sites.google.com/feeds/content/site/minimifidianism/6295486753647263806</id><published>2012-05-18T18:31:42.363Z</published><updated>2012-06-08T17:19:39.933Z</updated><app:edited xmlns:app="http://www.w3.org/2007/app">2012-06-08T17:19:39.310Z</app:edited><category scheme="http://schemas.google.com/g/2005#kind" term="http://schemas.google.com/sites/2008#announcement" label="announcement" /><title>Diablo 3 on Ubuntu Linux</title><content type="xhtml"><div xmlns="http://www.w3.org/1999/xhtml"><table cellspacing="0" class="sites-layout-name-one-column sites-layout-hbox"><tbody><tr><td class="sites-layout-tile sites-tile-name-content-1"><div dir="ltr">I sunk an obsessive amount of hours into Diablo and Diablo 2.  Now Diablo 3 is newly released and I cracked under the pressure.  I don't run Windows - I use Linux.  Ubuntu 12.04 Precise Pangolin, to be ... precise.  I also have an interesting set of criteria for whatever solution I find.<br />
<ul><li>I must not compile software.  I totally can do it, but I simply don't want to.</li>
<li>I want to use packages so when the upstream puts out a new version, I'm not left in the dust.</li>
<li>I want to be able to double-click on an icon when I'm done and Diablo 3 should launch.</li></ul>First off, we're going to have to use wine to run the Windows version of Diablo 3.  The version of wine in the repository won't work, so we need to add a custom PPA.  Another PPA will upgrade the video drivers for me.<div><br />
<div class="sites-codeblock sites-codesnippet-block"><code>sudo add-apt-repository ppa:cheako/packages4diabloiii</code><br />
<code>sudo add-apt-repository ppa:oibaf/graphics-drivers</code><br />
<code>sudo apt-get update</code></div>
<br />Now install the updated packages.  I also installed S3TC texture compression, which may be illegal where you are.</div><div><br />
<div class="sites-codeblock sites-codesnippet-block"><code>sudo apt-get upgrade</code><br />
<code>sudo apt-get install libtxc-dxtn0</code></div>
<br />Lastly, we'll need to tweak things a bit when we run wine.  First, go download the installer.  You can just double-click on it and it will install Diablo 3 and start downloading the gigs of data.  Once you get done with the download, or at least to a place where it will let you play the game, stop it.  Edit the link to Diablo 3.  Run "<code>gedit</code>" and edit <code>Desktop/Diablo III.desktop</code>.  Inside there, you will see a line that starts with "<code>Exec</code>".  Add the portion in bold below to force the use of S3TC.  Keep in mind that the next thing is all on one really long line.</div><div><br />
<div class="sites-codeblock sites-codesnippet-block"><code>Exec=env WINEPREFIX="/home/fidian/.wine" <b>force_s3tc_enable=true</b> wine C:\\\\windows\\\\command\\\\start.exe /Unix /home/fidian/.wine/dosdevices/c:/users/Public/Desktop/Diablo\\ III.lnk</code><br />
</div>
<br />Almost done.  Now we just need to disable some security.  You have two options: run a command as root whenever you want to run Diablo 3, or you can put it in your <code>/etc/rc.local</code> file and have it run automatically at boot.</div><div><br />
<div class="sites-codeblock sites-codesnippet-block"><code># Here is the command if you want to run it manually<br />
# Just run this once in a terminal<br />
echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope<br />
<br />
# If you want to edit /etc/rc.local (as root), add this line above "exit 0"<br />
# Edit the file with this command:  gksudo gedit /etc/rc.local<br />
echo 0 &gt; /proc/sys/kernel/yama/ptrace_scope</code><br />
</div>
<br />And now you can perhaps play.  I can't because the framerate is exceedingly slow, but perhaps that's just one last hurdle to getting the game to play.<br /></div></div></td></tr></tbody></table></div></content><link rel="http://schemas.google.com/sites/2008#parent" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/1715856284860793463" /><link rel="alternate" type="text/html" href="http://sites.google.com/site/minimifidianism/problems-only-tyler-has/diablo3onubuntulinux" /><link rel="http://schemas.google.com/sites/2008#revision" type="application/atom+xml" href="http://sites.google.com/feeds/revision/site/minimifidianism/6295486753647263806" /><link rel="self" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/6295486753647263806" /><link rel="edit" type="application/atom+xml" href="http://sites.google.com/feeds/content/site/minimifidianism/6295486753647263806" /><author><name>Tyler Akins</name><email>minimifidianism@gmail.com</email></author><sites:pageName>diablo3onubuntulinux</sites:pageName><sites:revision>4</sites:revision></entry></feed>
